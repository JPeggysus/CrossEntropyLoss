<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WTF is Cross-Entropy</title>
    <meta name="description" content="The Error Signal For Categorical AI Models">
    <meta name="google" content="0M1KqyRf8FC09Q7XlhtYTPWB9RaXwOWJZtC8SU2VI7M">
    <meta property="og:title" content="WTF is Cross-Entropy?">
    <meta property="og:description" content="The Error Signal For Categorical AI Models">
    <meta property="og:image" content="https://github.com/JPeggysus/CrossEntropyLoss/blob/main/preview.png?raw=true">
    <meta property="og:image" content="https://github.com/JPeggysus/CrossEntropyLoss/blob/main/preview.png?raw=true">
    <meta property="og:image:url" content="https://github.com/JPeggysus/CrossEntropyLoss/blob/main/preview.png?raw=true">
    <meta property="og:image:secure_url" content="https://github.com/JPeggysus/CrossEntropyLoss/blob/main/preview.png?raw=true">
    <meta property="og:image:secure_url" content="https://github.com/JPeggysus/CrossEntropyLoss/blob/main/preview.png?raw=true">
    <meta property="og:image:width" content="1202"><meta property="og:image:height" content="629">
    <meta property="og:type" content="website">
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Manrope:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --c-bg: #0f172a;
            --c-text: #cbd5e1;
            --c-text-muted: #94a3b8;
            --c-border: #334155;
            --c-card: rgba(15, 23, 42, 0.5);
            --c-brand-blue: #38bdf8;
            --c-brand-purple: #a78bfa;
            --c-brand-pink: #f472b6;
            --c-brand-green: #34d399;
            --c-brand-yellow: #facc15;
            --c-brand-red: #fb7185;
        }

        * { font-family: 'Manrope', sans-serif; }

        body {
            background-color: var(--c-bg);
            color: var(--c-text);
            overflow-x: hidden;
        }

        .mono { font-family: 'JetBrains Mono', monospace; }

        /* Animated gradient background */
        .gradient-bg {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: -1;
            background: var(--c-bg);
        }

        .gradient-orb {
            position: fixed;
            width: 50vw;
            height: 50vw;
            max-width: 600px;
            max-height: 600px;
            border-radius: 50%;
            filter: blur(120px);
            opacity: 0.2;
            pointer-events: none;
            transition: all 0.5s ease-in-out;
            z-index: -1;
        }

        .orb-1 {
            background: linear-gradient(45deg, var(--c-brand-blue), var(--c-brand-purple));
            top: 10vh;
            left: 10vw;
        }

        .orb-2 {
            background: linear-gradient(135deg, var(--c-brand-pink), var(--c-brand-yellow));
            bottom: 10vh;
            right: 10vw;
        }

        /* Section animations */
        .section {
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            padding: 6rem 1.5rem;
            position: relative;
            opacity: 0;
            transform: translateY(40px);
            transition: opacity 1s cubic-bezier(0.16, 1, 0.3, 1), transform 1s cubic-bezier(0.16, 1, 0.3, 1);
        }

        .section.visible {
            opacity: 1;
            transform: translateY(0);
        }

        /* Card styles */
        .glass-card {
            background: var(--c-card);
            backdrop-filter: blur(12px);
            border: 1px solid var(--c-border);
            border-radius: 1.5rem; /* Increased rounding */
            transition: all 0.4s ease;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
        }

        .glass-card:hover {
            border-color: var(--c-brand-blue);
            transform: translateY(-5px);
            box-shadow: 0 15px 40px rgba(0,0,0,0.15);
        }

        /* Progress indicator */
        .progress-bar {
            position: fixed;
            top: 0;
            left: 0;
            height: 3px;
            background: linear-gradient(90deg, var(--c-brand-blue), var(--c-brand-purple), var(--c-brand-pink));
            z-index: 1000;
            transition: width 0.2s ease-out;
            box-shadow: 0 0 10px var(--c-brand-purple);
        }

        /* Scroll indicator */
        .scroll-indicator {
            position: absolute;
            bottom: 2rem;
            left: 50%;
            transform: translateX(-50%);
            animation: bounce 2s infinite;
        }

        @keyframes bounce {
            0%, 20%, 50%, 80%, 100% { transform: translateX(-50%) translateY(0); }
            40% { transform: translateX(-50%) translateY(-20px); }
            60% { transform: translateX(-50%) translateY(-10px); }
        }

        /* Formula box */
        .formula-box {
            background: rgba(15, 23, 42, 0.7);
            border: 1px solid var(--c-border);
            border-left: 4px solid var(--c-brand-blue);
            padding: 2rem;
            border-radius: 0.75rem;
            font-family: 'JetBrains Mono', monospace;
            margin: 2rem 0;
        }

        /* Info callout */
        .info-callout {
            background: linear-gradient(135deg, rgba(56, 189, 248, 0.1), rgba(167, 139, 250, 0.1));
            border: 1px solid var(--c-brand-blue);
            border-radius: 1rem;
            padding: 1.5rem;
            margin: 2rem 0;
        }

        .info-callout-success {
            background: linear-gradient(135deg, rgba(52, 211, 153, 0.1), rgba(16, 185, 129, 0.1));
            border-color: var(--c-brand-green);
        }

        .info-callout-warning {
            background: linear-gradient(135deg, rgba(250, 204, 21, 0.1), rgba(245, 158, 11, 0.1));
            border-color: var(--c-brand-yellow);
        }

        /* Number display */
        .number-display {
            font-size: 2.75rem;
            font-weight: 700;
            font-family: 'JetBrains Mono', monospace;
            background: linear-gradient(135deg, var(--c-brand-blue), var(--c-brand-purple));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        /* Button */
        .btn-primary {
            background: linear-gradient(135deg, var(--c-brand-blue), var(--c-brand-purple));
            color: #ffffff;
            padding: 0.8rem 2.2rem;
            border-radius: 999px; /* Pill shape */
            font-weight: 600;
            border: none;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 5px 20px rgba(56, 189, 248, 0.3);
        }

        .btn-primary:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 30px rgba(56, 189, 248, 0.4);
        }

        /* Highlight text */
        .highlight-blue { color: var(--c-brand-blue); font-weight: 600; }
        .highlight-green { color: var(--c-brand-green); font-weight: 600; }
        .highlight-purple { color: var(--c-brand-purple); font-weight: 600; }
        .highlight-pink { color: var(--c-brand-pink); font-weight: 600; }
        .highlight-yellow { color: var(--c-brand-yellow); font-weight: 600; }
        .highlight-red { color: var(--c-brand-red); font-weight: 600; }

        /* Grid background */
        .grid-bg {
            background-image:
                linear-gradient(rgba(56, 189, 248, 0.05) 1px, transparent 1px),
                linear-gradient(90deg, rgba(56, 189, 248, 0.05) 1px, transparent 1px);
            background-size: 60px 60px;
        }

        /* Step indicator */
        .step-box {
            background: rgba(56, 189, 248, 0.05);
            border: 1px solid var(--c-border);
            border-radius: 1rem;
            padding: 1.5rem;
            margin: 1rem 0;
        }

        /* Progress through concept */
        .concept-progress {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin: 2rem 0;
            padding: 1rem;
        }

        .concept-step {
            flex: 1;
            text-align: center;
            padding: 1rem;
            border-radius: 0.75rem;
            background: rgba(30, 41, 59, 0.7);
            margin: 0 0.5rem;
            transition: all 0.3s ease;
            border: 1px solid var(--c-border);
        }

        .concept-step.active {
            background: rgba(56, 189, 248, 0.1);
            border-color: var(--c-brand-blue);
        }

        .concept-arrow {
            font-size: 2rem;
            color: var(--c-text-muted);
        }

        /* Scrollbar */
        ::-webkit-scrollbar { width: 8px; }
        ::-webkit-scrollbar-track { background: var(--c-bg); }
        ::-webkit-scrollbar-thumb {
            background: linear-gradient(180deg, var(--c-brand-blue), var(--c-brand-purple));
            border-radius: 4px;
        }

        /* Example box */
        .example-box {
            background: rgba(167, 139, 250, 0.05);
            border: 1px solid var(--c-border);
            border-left: 4px solid var(--c-brand-purple);
            padding: 1.5rem;
            border-radius: 0.75rem;
            margin: 1.5rem 0;
        }

        /* Float animation */
        @keyframes float {
            0%, 100% { transform: translateY(0px); }
            50% { transform: translateY(-20px); }
        }
        .float { animation: float 6s ease-in-out infinite; }
    </style>
</head>
<body class="grid-bg">
    <!-- Progress bar -->
    <div class="progress-bar" id="progressBar"></div>

    <!-- Gradient background -->
    <div class="gradient-bg"></div>
    <div class="gradient-orb orb-1"></div>
    <div class="gradient-orb orb-2"></div>

    <!-- Main content -->
    <div class="relative">

        <!-- SECTION 0: Hero -->
        <section class="section visible" id="section-0">
            <div class="max-w-5xl mx-auto text-center">
                </div>
                <h1 class="text-6xl md:text-8xl font-black mb-6 bg-gradient-to-r from-blue-400 via-purple-400 to-pink-400 bg-clip-text text-transparent">
                    <br>WTF is Cross-Entropy
                </h1>
                <h1 class="text-6xl md:text-8xl font-black mb-6 bg-gradient-to-r from-blue-400 via-purple-400 to-pink-400 bg-clip-text text-transparent">
                    Loss?
                </h1>
                <p class="text-2xl md:text-3xl text-gray-400 mb-8 font-light">
                    A Step-by-Step Look Into
                </p>
                <p class="text-3xl md:text-4xl font-bold mb-12">
                    <span class="highlight-blue">Entropy</span>,
                    <span class="highlight-purple">Cross-Entropy</span>, and
                    <span class="highlight-pink">KL Divergence</span>
                </p>
                <div class="max-w-3xl mx-auto mb-12">
                    <div class="info-callout">
                        <p class="text-lg leading-relaxed">
                            When training models for classification tasks, we seek to minimize "cross-entropy loss". But what does this number actually mean?
                            How does it represent our network's prediction error?
                        </p>
                    </div>
                </div>
                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 1: What Neural Networks Output -->
        <section class="section" id="section-1">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 1: What Does a Classifier Model <span class="highlight-blue">Output</span>?
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                   We have a computer vision model that is trained to label images.
                    If we input an image of a cat, what comes out?
                </p>

                <div class="glass-card p-8 mb-8">
                    <div class="text-center mb-6">
                        <div class="text-6xl mb-4">üê±</div>
                        <p class="text-gray-400 text-sm">Input: A photo that's definitely a cat</p>
                    </div>

                    <div class="example-box">
                        <p class="font-bold mb-4">‚ùå What it does NOT output:</p>
                        <p class="text-gray-300 mb-2">A single answer like: <span class="mono text-red-400">"CAT"</span></p>
                    </div>

                    <div class="example-box">
                        <p class="font-bold mb-4">‚úÖ What it ACTUALLY outputs:</p>
                        <p class="text-gray-300 mb-4">A probability for EVERY possible label:</p>

                        <div class="space-y-3">
                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê± Cat</span>
                                    <span class="mono text-blue-400 font-bold">70%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-gradient-to-r from-blue-500 to-blue-400 rounded-full" style="width: 70%"></div>
                                </div>
                            </div>

                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê∂ Dog</span>
                                    <span class="mono text-blue-400 font-bold">25%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-gradient-to-r from-blue-500 to-blue-400 rounded-full" style="width: 25%"></div>
                                </div>
                            </div>

                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê¶ Bird</span>
                                    <span class="mono text-blue-400 font-bold">5%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-gradient-to-r from-blue-500 to-blue-400 rounded-full" style="width: 5%"></div>
                                </div>
                            </div>
                        </div>

                        <div class="mt-4 pt-4 border-t border-gray-600">
                            <p class="text-center text-sm">
                                <span class="text-gray-400">Total: </span>
                                <span class="text-white font-bold">70% + 25% + 5% = 100%</span>
                            </p>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-lg">
                        <strong>Key Point:</strong> The output is a <span class="highlight-green">probability distribution</span>.
                        The model gives a probability for each possible answer, and they all sum to 100%.
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 2: Why Distributions? -->
        <section class="section" id="section-2">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 2: Why a <span class="highlight-purple">Distribution</span>?
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Why doesn't the model just pick one answer? Why give probabilities?
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">The Answer: <span class="highlight-yellow">Uncertainty</span></h3>

                    <p class="text-gray-300 mb-6 text-lg">
                        The model is essentially making an educated guess. It doesn't have perfect information,
                        so it expresses its <span class="highlight-yellow">confidence</span> about each possibility.
                    </p>

                    <div class="grid md:grid-cols-2 gap-6 mb-6">
                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg">
                            <h4 class="font-bold text-green-400 mb-3 text-center">High Confidence</h4>
                            <div class="text-center mb-4">
                                <p class="mono text-3xl text-green-400">95% Cat</p>
                                <p class="text-sm text-gray-400">5% other things</p>
                            </div>
                            <p class="text-sm text-gray-300">
                                "I'm very sure this is a cat. The features are clear."
                            </p>
                        </div>

                        <div class="p-6 bg-yellow-900/20 border-2 border-yellow-500/30 rounded-lg">
                            <h4 class="font-bold text-yellow-400 mb-3 text-center">Low Confidence</h4>
                            <div class="text-center mb-4">
                                <p class="mono text-3xl text-yellow-400">40% Cat</p>
                                <p class="text-sm text-gray-400">60% other things</p>
                            </div>
                            <p class="text-sm text-gray-300">
                                "It might be a cat, but I'm not sure. It's blurry or ambiguous."
                            </p>
                        </div>
                    </div>
                </div>

                <div class="info-callout">
                    <p class="text-lg">
                        The model's prediction distibution (<span class="highlight-blue">Q</span>)
                        represents its <span class="highlight-blue">beliefs</span>
                        about what the image might be.
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 3: The Ground Truth -->
        <section class="section" id="section-3">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 3: The <span class="highlight-green">Ground Truth</span>
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    We know the correct answer. How do we represent that?
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">The Correct Label as a Distribution</h3>

                    <p class="text-gray-300 mb-6 text-lg">
                        Since we know the image is definitely a cat, we represent that certainty as:
                    </p>

                    <div class="example-box">
                        <div class="space-y-3">
                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê± Cat (the correct answer)</span>
                                    <span class="mono text-green-400 font-bold">100%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-green-500 rounded-full" style="width: 100%"></div>
                                </div>
                            </div>

                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê∂ Dog</span>
                                    <span class="mono text-gray-400 font-bold">0%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-gray-600 rounded-full" style="width: 0%"></div>
                                </div>
                            </div>

                            <div>
                                <div class="flex justify-between text-sm mb-1">
                                    <span>üê¶ Bird</span>
                                    <span class="mono text-gray-400 font-bold">0%</span>
                                </div>
                                <div class="w-full bg-gray-700/50 rounded-full h-6">
                                    <div class="h-full bg-gray-600 rounded-full" style="width: 0%"></div>
                                </div>
                            </div>
                        </div>

                        <div class="mt-4 pt-4 border-t border-gray-600">
                            <p class="text-center text-sm">
                                <span class="text-gray-400">Total: </span>
                                <span class="text-white font-bold">100% + 0% + 0% = 100%</span>
                            </p>
                        </div>
                    </div>

                    <p class="text-gray-300 mt-6">
                        This is called a <span class="highlight-green">"one-hot encoded"</span> distribution.
                        One class gets 100%, all others get 0%.
                    </p>
                </div>

                <div class="info-callout">
                    <p class="text-lg">
                        Notice that the ground truth is also a distribution. The correct label is assigned 100% probability while every other is assigned 0%.
                        This true distribution (<span class="highlight-green">P</span>) represents <span class="highlight-green">reality</span>, i.e.
                        the accurate label with absolute certainty.
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 4: The Problem -->
        <section class="section" id="section-4">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 4: The <span class="highlight-pink">Problem</span> We Need to Solve
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    With our model's prediction and the ground truth both represented as distributions across all possible labels,
                    how do we measure how "wrong" the model is?
                </p>

                <div class="grid md:grid-cols-3 gap-6 mb-12">
                    <div class="glass-card p-6 text-center">
                        <div class="text-4xl mb-3">ü§ñ</div>
                        <h4 class="font-bold text-blue-400 mb-2">Model's Prediction (Q)</h4>
                        <div class="space-y-2 text-sm">
                            <p>Cat: <span class="mono">60%</span></p>
                            <p>Dog: <span class="mono">30%</span></p>
                            <p>Bird: <span class="mono">10%</span></p>
                        </div>
                    </div>

                    <div class="flex items-center justify-center">
                        <div class="text-center">
                            <div class="text-4xl text-pink-400 mb-2">‚öñÔ∏è</div>
                            <p class="text-gray-400 font-bold">How different?</p>
                        </div>
                    </div>

                    <div class="glass-card p-6 text-center">
                        <div class="text-4xl mb-3">‚úÖ</div>
                        <h4 class="font-bold text-green-400 mb-2">Ground Truth (P)</h4>
                        <div class="space-y-2 text-sm">
                            <p>Cat: <span class="mono">100%</span></p>
                            <p>Dog: <span class="mono">0%</span></p>
                            <p>Bird: <span class="mono">0%</span></p>
                        </div>
                    </div>
                </div>
                <div class="info-callout info-callout-warning">
                    <p class="text-lg">
                        <strong>The Goal:</strong> We need to find a calculation that tells us how different
                        the model's prediction <span class="highlight-blue">(Q)</span> is compared to the truth <span class="highlight-green">(P)</span>. 
                        Rather than looking at just the correct label,
                        this number should take into account both distributions in their entirety and be:
                        <br><br>
                        ‚Ä¢ Zero when the model's prediction perfectly matches reality<br>
                        ‚Ä¢ Smaller when Q is close to P<br>
                        ‚Ä¢ Larger when Q is far P
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 5: Building Block - Surprise -->
        <section class="section" id="section-5">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 5: Understanding <span class="highlight-yellow">Surprise</span>
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    The backbone of our methodology for determining how incorrect our model's predictions are is a concept from information theory called "Surprise"
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">What is Surprise?</h3>

                    <p class="text-gray-300 mb-6 text-lg">
                        A way to numerically represent how unexpected an event is. Whenever, we assign a probability to a potential outcome, we are saying 
                        "I believe with X% certainty that this will happen". We can take this level of certainty and quantify how unexpected this event would be if it were to happen.
                    </p>

                    <div class="grid md:grid-cols-2 gap-6 mb-6">
                        <div class="p-6 bg-blue-900/20 border-2 border-blue-500/30 rounded-lg">
                            <h4 class="font-bold text-blue-400 mb-3 text-center">High Probability Event</h4>
                            <div class="text-center mb-4">
                                <p class="text-sm text-gray-400 mb-2">If you expect something to happen:</p>
                                <p class="mono text-2xl text-blue-400">p = 99%</p>
                            </div>
                            <p class="text-center text-3xl mb-3">üòê</p>
                            <p class="text-sm text-gray-300 text-center">
                                <span class="highlight-blue">Low surprise</span> when it actually happens<br>
                                "Yeah, I expected that."
                            </p>
                        </div>

                        <div class="p-6 bg-red-900/20 border-2 border-red-500/30 rounded-lg">
                            <h4 class="font-bold text-red-400 mb-3 text-center">Low Probability Event</h4>
                            <div class="text-center mb-4">
                                <p class="text-sm text-gray-400 mb-2">If you expect something is rare:</p>
                                <p class="mono text-2xl text-red-400">p = 1%</p>
                            </div>
                            <p class="text-center text-3xl mb-3">üò≤</p>
                            <p class="text-sm text-gray-300 text-center">
                                <span class="highlight-red">High surprise</span> when it actually happens<br>
                                "Whoa! I didn't expect that!"
                            </p>
                        </div>
                    </div>
                </div>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">The Surprise Formula</h3>

                    <p class="text-gray-300 mb-6">
                        We can quantify surprise mathematically by using the properties of logarithms. In ML we use the natural log for its conveneint properties
                        and represent surprise in the unit "nats". In Information Theory, we use log base 2 and "bits". 
                        This isn't crucial to know, but is potentially useful if you choose to dive deeper into these topics. 
                        <br><br>For the sake of simplicity, 
                        all that is crucial for you to understand is when the probability (p) of an event goes down, suprise goes up. When p goes up, surprise goes down.
                    </p>

                    <div class="formula-box text-center">
                        <p class="text-sm text-gray-400 mb-2">Surprise of an event with probability p:</p>
                        <p class="text-2xl mt-4">
                            <span class="text-white">Surprise = </span>
                            <span class="text-yellow-400 font-bold">ln(1/p)</span>
                        </p>
                        <p class="text-sm text-gray-400 mt-4">Also written as:</p>
                        <p class="text-2xl mt-2">
                            <span class="text-white">Surprise = </span>
                            <span class="text-yellow-400 font-bold">-ln(p)</span>
                        </p>
                    </div>

                    <div class="mt-8 space-y-4">
                        <h4 class="font-bold text-center mb-4 text-xl">üéÆ Interactive Example</h4>
                        <div class="p-6 bg-gray-900/30 rounded-lg border border-slate-700">
                            <div class="flex items-center justify-between mb-4">
                                <label for="surprise-slider" class="font-bold text-lg">Event Probability (p):</label>
                                <span id="surprise-prob-display" class="mono text-2xl font-bold text-blue-400">50%</span>
                            </div>
                            <input type="range" id="surprise-slider" min="1" max="99" value="50" class="w-full h-3 rounded-lg appearance-none cursor-pointer bg-slate-700">

                            <div class="mt-6 text-center">
                                <p class="text-sm text-gray-400 mb-2">Resulting Surprise:</p>
                                <p id="surprise-value-display" class="number-display text-yellow-400" style="font-size: 2.5rem;">0.693 nats</p>
                                <p id="surprise-text-display" class="text-lg text-gray-300 mt-2">Moderately surprised</p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-lg">
                        <strong>Key Insight:</strong> Surprise increases as probability decreases.
                        Rare events are surprising, common events are not. The logarithm gives us a nice
                        mathematical way to quantify this intuition.
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 6: Entropy - Average Surprise -->
        <section class="section" id="section-6">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 6: <span class="highlight-purple">Entropy</span> = Average Surprise
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Now we know how to calculate surprise for one event. But what about an entire distribution?
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">From Surprise to Entropy</h3>

                    <p class="text-gray-300 mb-6 text-lg">
                        A distribution has many possible outcomes, each with its own probability and surprise.
                        <span class="highlight-purple">Entropy</span> is the <span class="highlight-yellow">average surprise</span>
                        you'd expect if you sampled from this distribution. <br><br>Entropy answers: "How much suprise can I expect, given this probability distribution of events?"
                    </p>

                    <div class="example-box">
                        <h4 class="font-bold mb-4">Example: A Coin Flip</h4>
                        <p class="text-gray-300 mb-4">Let's say we have a fair coin (50% heads, 50% tails):</p>

                        <div class="grid md:grid-cols-2 gap-4 mb-4">
                            <div class="p-4 bg-gray-800/50 rounded-lg">
                                <p class="font-bold text-blue-400 mb-2">Heads</p>
                                <p class="text-sm">Probability: <span class="mono">0.5</span></p>
                                <p class="text-sm">Surprise: <span class="mono">-ln(0.5) = 0.693 nats</span></p>
                            </div>
                            <div class="p-4 bg-gray-800/50 rounded-lg">
                                <p class="font-bold text-blue-400 mb-2">Tails</p>
                                <p class="text-sm">Probability: <span class="mono">0.5</span></p>
                                <p class="text-sm">Surprise: <span class="mono">-ln(0.5) = 0.693 nats</span></p>
                            </div>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-2">To get average surprise (Entropy):</p>
                            <p class="text-sm text-gray-300">Weight each surprise by how likely it is to happen:</p>
                            <div class="mt-3 p-3 bg-gray-900/50 rounded">
                                <p class="mono text-sm">
                                    H(P) = (0.5 √ó 0.693) + (0.5 √ó 0.693) = 0.693 nats
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">The General Entropy Formula</h3>

                    <div class="formula-box text-center">
                        <p class="text-sm text-gray-400 mb-4">For a distribution P with outcomes i:</p>
                        <p class="text-2xl">
                            <span class="text-white">H(P) = </span>
                            <span class="text-purple-400">Œ£</span>
                            <span class="text-white"> [ </span>
                            <span class="text-green-400">p(i)</span>
                            <span class="text-white"> √ó </span>
                            <span class="text-yellow-400">(-ln p(i))</span>
                            <span class="text-white"> ]</span>
                        </p>
                        <div class="mt-6 text-sm text-gray-400">
                            <p><span class="text-green-400">p(i)</span> = probability of outcome i</p>
                            <p><span class="text-yellow-400">-ln p(i)</span> = surprise of outcome i</p>
                            <p><span class="text-purple-400">Œ£</span> = sum over all possible outcomes</p>
                        </div>
                    </div>

                    <p class="text-gray-300 mt-6 text-center">
                        In plain English: <span class="highlight-purple">"For each possible outcome, multiply its probability
                        by its surprise, then add them all up."</span>
                    </p>
                </div>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">üéÆ Interactive Coin Flip Example</h3>

                    <div class="p-6 bg-gray-900/30 rounded-lg border border-slate-700">
                        <div class="flex items-center justify-between mb-4">
                            <label for="entropy-slider" class="font-bold text-lg">Adjust Coin Fairness:</label>
                            <span id="entropy-prob-display" class="mono text-2xl font-bold text-purple-400">50% Heads</span>
                        </div>
                        <input type="range" id="entropy-slider" min="1" max="99" value="50" class="w-full h-3 rounded-lg appearance-none cursor-pointer bg-slate-700">

                        <div class="grid md:grid-cols-2 gap-6 mt-6">
                            <div>
                                <h4 class="font-bold text-center mb-4">Coin Probabilities</h4>
                                <div class="space-y-4">
                                    <div>
                                        <p class="text-sm mb-1 flex justify-between"><span>Heads:</span> <span id="heads-prob-text">50%</span></p>
                                        <div class="w-full bg-gray-700/50 rounded-full h-4">
                                            <div id="heads-prob-bar" class="bg-purple-500 h-4 rounded-full" style="width: 50%"></div>
                                        </div>
                                    </div>
                                    <div>
                                        <p class="text-sm mb-1 flex justify-between"><span>Tails:</span> <span id="tails-prob-text">50%</span></p>
                                        <div class="w-full bg-gray-700/50 rounded-full h-4">
                                            <div id="tails-prob-bar" class="bg-blue-500 h-4 rounded-full" style="width: 50%"></div>
                                        </div>
                                    </div>
                                </div>
                            </div>

                            <div class="text-center">
                                <h4 class="font-bold mb-4">Resulting Entropy</h4>
                                <div class="p-4 bg-gray-900/50 rounded-lg">
                                    <p id="entropy-value-display" class="number-display text-purple-400" style="font-size: 2.5rem;">0.693</p>
                                    <p class="text-xs text-gray-400">nats</p>
                                </div>
                                <p id="entropy-text-display" class="text-sm text-gray-300 mt-3">
                                    High uncertainty - you don't know what you'll get!
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-lg">
                        <strong>Key Takeaway:</strong>
                        <span class="highlight-purple">Entropy measures uncertainty</span> in a distribution.
                        <br><br>
                        ‚Ä¢ High entropy = unpredictable (fair coin)<br>
                        ‚Ä¢ Low entropy = predictable (unfair coin)<br>
                        ‚Ä¢ Zero entropy = completely certain (only one outcome possible)
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 7: Cross-Entropy - Wrong Beliefs -->
        <section class="section" id="section-7">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 7: <span class="highlight-pink">Cross-Entropy</span> = Surprise with Wrong Beliefs
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    <span class="highlight-pink">What if our beliefs don't match reality?</span>
                </p>
                <div class="glass-card p-8 mb-8">
                    <p class="text-gray-300 mb-6 text-lg">
                        When we have a known distribution of events, such as the flip of a fair coin, we can calculate its entropy. We saw this in the previous step.
                        However, rarely do we have complete information. The ground truth reality is typically unknown to us. For instance, the coin we flip may be rigged. As a result, our expectations of events
                        are typically based on belief. Our own proability distribution of events is likely different from the true one.<br><br>
                        We believe the coin to be fair, when in reality it may not be. As a result, innacuracies in our beliefs can cause us to be more or less surprised
                        by outcomes than we would've been if we had known their true probabilities. <br><br>

                        Can we determine how much surprise to expect from an event governed by the ground truth probability distribution (<span class="highlight-green">P</span>), while believing  probability distribution (<span class="highlight-blue">Q</span>)?
                        <br><br>
                        Absolutely, by using <span class="highlight-pink">Cross-Entropy</span>
                    <p
                </div>
                <!-- COMPARISON: ENTROPY VS CROSS-ENTROPY -->
                <div class="glass-card p-8 mb-8 bg-gradient-to-br from-purple-900/20 to-pink-900/20 border-2 border-pink-500/30">
                    <h3 class="text-3xl font-bold mb-8 text-center">Entropy vs Cross-Entropy</h3>

                    <div class="grid md:grid-cols-2 gap-6 mb-8">
                        <div class="p-6 bg-purple-900/30 border-2 border-purple-500/50 rounded-lg">
                            <h4 class="text-2xl font-bold text-purple-400 mb-4 text-center">Entropy H(P)</h4>
                            <p class="text-center text-gray-300 mb-6 text-lg">
                                "Surprise when you know the truth"
                            </p>

                            <div class="space-y-4 text-sm">
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-purple-400 mb-2">Your beliefs:</p>
                                    <p class="text-white">Distribution P (the truth)</p>
                                </div>
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-purple-400 mb-2">Reality:</p>
                                    <p class="text-white">Distribution P (the truth)</p>
                                </div>
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-purple-400 mb-2">Your surprise:</p>
                                    <p class="text-white">Based on P (accurate!)</p>
                                </div>
                            </div>

                            <div class="mt-6 p-4 bg-purple-900/40 rounded-lg">
                                <p class="text-center font-mono text-lg">
                                    H(P) = Œ£ [ <span class="text-purple-400">p(i)</span> √ó (-ln <span class="text-purple-400">p(i)</span>) ]
                                </p>
                                <p class="text-center text-xs text-gray-400 mt-2">
                                    Same distribution for beliefs AND reality
                                </p>
                            </div>
                        </div>

                        <div class="p-6 bg-pink-900/30 border-2 border-pink-500/50 rounded-lg">
                            <h4 class="text-2xl font-bold text-pink-400 mb-4 text-center">Cross-Entropy H(P,Q)</h4>
                            <p class="text-center text-gray-300 mb-6 text-lg">
                                "Surprise when your beliefs are wrong"
                            </p>

                            <div class="space-y-4 text-sm">
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-blue-400 mb-2">Your beliefs:</p>
                                    <p class="text-white">Distribution Q (your guess)</p>
                                </div>
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-green-400 mb-2">Reality:</p>
                                    <p class="text-white">Distribution P (the truth)</p>
                                </div>
                                <div class="p-4 bg-gray-900/50 rounded">
                                    <p class="font-bold text-yellow-400 mb-2">Your surprise:</p>
                                    <p class="text-white">Based on Q (inaccurate!)</p>
                                </div>
                            </div>

                            <div class="mt-6 p-4 bg-pink-900/40 rounded-lg">
                                <p class="text-center font-mono text-lg">
                                    H(P,Q) = Œ£ [ <span class="text-green-400">p(i)</span> √ó (-ln <span class="text-blue-400">q(i)</span>) ]
                                </p>
                                <p class="text-center text-xs text-gray-400 mt-2">
                                    Different distributions: reality ‚â† beliefs
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="p-6 bg-gradient-to-r from-yellow-900/30 to-red-900/30 rounded-lg border-2 border-yellow-500/50">
                        <p class="text-2xl font-bold text-center mb-4">The Critical Difference</p>
                        <p class="text-lg text-center text-gray-200 leading-relaxed">
                            In <span class="highlight-purple">entropy</span>, your surprise matches reality because your expectations are based on the true probabilities.
                            <br><br>
                            In <span class="highlight-pink">cross-entropy</span>, you calculate surprise using your <span class="highlight-blue">beliefs (Q)</span>,
                            but events actually happen according to <span class="highlight-green">reality (P)</span>.
                            This mismatch creates <span class="highlight-yellow">extra surprise</span>!
                        </p>
                    </div>
                </div>

                <!-- CONCRETE EXAMPLE -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">ü™ô Concrete Example: The Weighted Coin</h3>

                    <div class="grid md:grid-cols-2 gap-6 mb-6">
                        <div class="p-6 bg-blue-900/20 border-2 border-blue-500/30 rounded-lg">
                            <h4 class="font-bold text-blue-400 mb-3 text-center">Your Belief (Q)</h4>
                            <p class="text-sm text-gray-300 mb-4 text-center">
                                You <span class="highlight-blue">think</span> the coin is fair
                            </p>
                            <div class="space-y-3">
                                <div>
                                    <p class="text-sm mb-1">Heads: <span class="mono">50%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-blue-500 h-4 rounded-full" style="width: 50%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm mb-1">Tails: <span class="mono">50%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-blue-500 h-4 rounded-full" style="width: 50%"></div>
                                    </div>
                                </div>
                            </div>
                            <div class="mt-4 p-3 bg-blue-900/30 rounded text-center">
                                <p class="text-sm text-gray-300">Based on Q, you calculate:</p>
                                <p class="mono text-blue-400 text-lg font-bold">Surprise(Heads) = -ln(0.5) = 0.693 nats</p>
                            </div>
                        </div>

                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg">
                            <h4 class="font-bold text-green-400 mb-3 text-center">Reality (P)</h4>
                            <p class="text-sm text-gray-300 mb-4 text-center">
                                The coin is <span class="highlight-green">actually</span> weighted
                            </p>
                            <div class="space-y-3">
                                <div>
                                    <p class="text-sm mb-1">Heads: <span class="mono">99%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-green-500 h-4 rounded-full" style="width: 99%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm mb-1">Tails: <span class="mono">1%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-green-500 h-4 rounded-full" style="width: 1%"></div>
                                    </div>
                                </div>
                            </div>
                            <div class="mt-4 p-3 bg-green-900/30 rounded text-center">
                                <p class="text-sm text-gray-300">But Heads happens:</p>
                                <p class="mono text-green-400 text-lg font-bold">99% of the time!</p>
                            </div>
                        </div>
                    </div>

                    <div class="info-callout info-callout-warning">
                        <p class="text-lg mb-4">
                            <strong>The Key Insight:</strong>
                        </p>
                        <p class="text-gray-200 text-lg leading-relaxed">
                            You flip the coin 100 times. Heads comes up 99 times (because reality is P).
                            <br><br>
                            Each time you see Heads, you experience <span class="highlight-blue">0.693 nats of surprise</span>
                            (because you believe Q says it's 50/50).
                            <br><br>
                            But if you knew the truth (P), you'd only experience <span class="highlight-green">0.01 nats of surprise</span>
                            per Heads (because you'd know it's 99% likely).
                            <br><br>
                            <span class="highlight-yellow text-xl">Cross-entropy captures this extra surprise from being wrong!</span>
                        </p>
                    </div>
                </div>

                <!-- THE FORMULA -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">The Cross-Entropy Formula Explained</h3>

                    <div class="formula-box text-center">
                        <p class="text-sm text-gray-400 mb-4">Cross-Entropy between reality (P) and beliefs (Q):</p>
                        <p class="text-2xl mb-6">
                            <span class="text-white">H(P, Q) = </span>
                            <span class="text-purple-400">Œ£</span>
                            <span class="text-white"> [ </span>
                            <span class="text-green-400">p(i)</span>
                            <span class="text-white"> √ó </span>
                            <span class="text-blue-400">(-ln q(i))</span>
                            <span class="text-white"> ]</span>
                        </p>
                    </div>

                    <div class="grid md:grid-cols-3 gap-4 mt-6 mb-6">
                        <div class="p-4 bg-green-900/30 border border-green-500/50 rounded-lg">
                            <p class="text-center font-bold text-green-400 mb-2">p(i)</p>
                            <p class="text-center text-sm text-gray-300">
                                How often outcome i <span class="font-bold">actually happens</span> (reality)
                            </p>
                        </div>
                        <div class="p-4 bg-blue-900/30 border border-blue-500/50 rounded-lg">
                            <p class="text-center font-bold text-blue-400 mb-2">-ln q(i)</p>
                            <p class="text-center text-sm text-gray-300">
                                How surprised <span class="font-bold">you are</span> based on your beliefs
                            </p>
                        </div>
                        <div class="p-4 bg-yellow-900/30 border border-yellow-500/50 rounded-lg">
                            <p class="text-center font-bold text-yellow-400 mb-2">p(i) √ó (-ln q(i))</p>
                            <p class="text-center text-sm text-gray-300">
                                <span class="font-bold">Expected surprise</span> for outcome i
                            </p>
                        </div>
                    </div>

                    <div class="p-6 bg-gray-900/50 rounded-lg">
                        <p class="font-bold text-center mb-4 text-xl">In Plain English:</p>
                        <p class="text-gray-200 text-center text-lg leading-relaxed">
                            "For each possible outcome, take how often it <span class="text-green-400">actually happens</span> in reality (p),
                            multiply by how <span class="text-blue-400">surprised you are</span> based on your wrong beliefs (using q),
                            then add it all up to get your <span class="text-yellow-400">average surprise</span>."
                        </p>
                    </div>
                </div>

                <!-- CALCULATE THE EXAMPLE -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">Let's Calculate Our Coin Example</h3>

                    <div class="space-y-6">
                        <div class="step-box">
                            <p class="font-bold mb-3">Step 1: Calculate surprise for each outcome based on your beliefs (Q)</p>
                            <div class="grid md:grid-cols-2 gap-4">
                                <div class="p-3 bg-gray-800/50 rounded">
                                    <p class="text-sm">Heads: q = 0.5</p>
                                    <p class="mono text-blue-400">-ln(0.5) = 0.693 nats</p>
                                    <p class="text-xs text-gray-400 mt-1">"I'd be moderately surprised"</p>
                                </div>
                                <div class="p-3 bg-gray-800/50 rounded">
                                    <p class="text-sm">Tails: q = 0.5</p>
                                    <p class="mono text-blue-400">-ln(0.5) = 0.693 nats</p>
                                    <p class="text-xs text-gray-400 mt-1">"I'd be moderately surprised"</p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3">Step 2: Weight by how often each actually happens (P)</p>
                            <div class="p-4 bg-gray-800/50 rounded">
                                <p class="mono text-sm mb-2">H(P, Q) = <span class="text-green-400">p(Heads)</span> √ó <span class="text-blue-400">Surprise(Heads)</span> + <span class="text-green-400">p(Tails)</span> √ó <span class="text-blue-400">Surprise(Tails)</span></p>
                                <p class="mono text-sm mb-2">H(P, Q) = <span class="text-green-400">(0.99)</span> √ó <span class="text-blue-400">(0.693)</span> + <span class="text-green-400">(0.01)</span> √ó <span class="text-blue-400">(0.693)</span></p>
                                <p class="mono text-sm">H(P, Q) = 0.686 + 0.007 = <span class="text-red-400 font-bold text-lg">0.693 nats</span></p>
                            </div>
                            <p class="text-sm text-gray-400 mt-2 text-center">Your average surprise per flip (with wrong beliefs)</p>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3">Step 3: Compare to entropy if you knew the truth</p>
                            <div class="p-4 bg-gray-800/50 rounded">
                                <p class="mono text-sm mb-2">H(P) = <span class="text-green-400">p(Heads)</span> √ó <span class="text-green-400">(-ln p(Heads))</span> + <span class="text-green-400">p(Tails)</span> √ó <span class="text-green-400">(-ln p(Tails))</span></p>
                                <p class="mono text-sm mb-2">H(P) = (0.99) √ó (0.01) + (0.01) √ó (4.605)</p>
                                <p class="mono text-sm">H(P) = <span class="text-green-400 font-bold text-lg">0.056 nats</span></p>
                            </div>
                            <p class="text-sm text-gray-400 mt-2 text-center">Average surprise if you knew the truth</p>
                        </div>

                        <div class="p-6 bg-gradient-to-r from-red-900/30 to-yellow-900/30 rounded-lg border-2 border-yellow-500/30">
                            <p class="font-bold text-center text-2xl mb-4">The Result:</p>
                            <div class="text-center space-y-3">
                                <p class="text-xl">
                                    <span class="text-red-400 font-bold">Cross-Entropy: 0.693 nats</span>
                                    <span class="text-gray-400 mx-3">></span>
                                    <span class="text-green-400 font-bold">Entropy: 0.056 nats</span>
                                </p>
                                <p class="text-lg text-gray-200">
                                    You experience <span class="text-yellow-400 font-bold text-2xl">0.637 extra nats</span> of surprise!
                                </p>
                                <p class="text-gray-300">
                                    This extra surprise is the "penalty" for having wrong beliefs (Q) about reality (P).
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-2xl mb-4">
                        <strong>Summary - Entropy vs Cross-Entropy:</strong>
                    </p>
                    <div class="space-y-3 text-lg">
                        <p class="text-gray-200">
                            <span class="highlight-purple">Entropy H(P):</span> The average surprise when you know the true distribution P.
                            This is the minimum possible surprise. It is entirely due to reality's inherent uncertainty.
                        </p>
                        <p class="text-gray-200">
                            <span class="highlight-pink">Cross-Entropy H(P,Q):</span> The average surprise when reality follows distribution P,
                            but you're expecting distribution Q. Always ‚â• H(P), with the difference being your "wrongness penalty."
                        </p>
                        <p class="text-yellow-400 font-bold text-xl text-center mt-6">
                            When P = Q (beliefs match reality): Cross-Entropy = Entropy ‚úì<br>
                            When P ‚â† Q (beliefs don't match): Cross-Entropy > Entropy ‚úó
                        </p>
                    </div>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 8: KL Divergence -->
        <section class="section" id="section-8">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 8: <span class="highlight-yellow">KL Divergence</span> = The Extra Surprise
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Now we can isolate exactly how much "extra" surprise comes from having wrong beliefs.
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">Breaking Down the Surprise</h3>

                    <div class="space-y-6">
                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg">
                            <div class="flex items-center justify-between mb-3">
                                <h4 class="font-bold text-green-400">Entropy H(P)</h4>
                                <span class="mono text-2xl text-green-400">0.056 nats</span>
                            </div>
                            <p class="text-sm text-gray-300">
                                The <span class="highlight-green">inherent uncertainty</span> in reality itself.
                                This is the minimum surprise you'd expect if you knew the true distribution perfectly.
                            </p>
                        </div>

                        <div class="text-center text-4xl text-gray-500">+</div>

                        <div class="p-6 bg-yellow-900/20 border-2 border-yellow-500/30 rounded-lg">
                            <div class="flex items-center justify-between mb-3">
                                <h4 class="font-bold text-yellow-400">KL Divergence D<sub>KL</sub>(P || Q)</h4>
                                <span class="mono text-2xl text-yellow-400">0.637 nats</span>
                            </div>
                            <p class="text-sm text-gray-300">
                                The <span class="highlight-yellow">additional surprise</span> expected from having wrong beliefs.
                                This is the "penalty" for Q being different from P.
                            </p>
                        </div>

                        <div class="text-center text-4xl text-gray-500">=</div>

                        <div class="p-6 bg-red-900/20 border-2 border-red-500/30 rounded-lg">
                            <div class="flex items-center justify-between mb-3">
                                <h4 class="font-bold text-red-400">Cross-Entropy H(P, Q)</h4>
                                <span class="mono text-2xl text-red-400">0.693 nats</span>
                            </div>
                            <p class="text-sm text-gray-300">
                                The <span class="highlight-red">total surprise</span> you experience when reality is P
                                but you believe Q.
                            </p>
                        </div>
                    </div>

                    <div class="formula-box mt-8 text-center">
                        <p class="text-sm text-gray-400 mb-4">The relationship:</p>
                        <p class="text-2xl">
                            <span class="text-yellow-400 font-bold">D<sub>KL</sub>(P || Q)</span>
                            <span class="text-white"> = </span>
                            <span class="text-red-400">H(P, Q)</span>
                            <span class="text-white"> - </span>
                            <span class="text-green-400">H(P)</span>
                        </p>
                        <p class="text-sm text-gray-400 mt-4">In words:</p>
                        <p class="text-base text-gray-300 mt-2">
                            KL Divergence = Cross-Entropy - Entropy
                        </p>
                    </div>
                </div>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">What Does KL Divergence Tell Us?</h3>

                    <div class="grid md:grid-cols-3 gap-6">
                        <div class="p-6 bg-gray-800/50 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚úÖ</div>
                            <p class="font-bold text-green-400 mb-2">Perfect Match</p>
                            <p class="mono text-2xl mb-2">D<sub>KL</sub> = 0</p>
                            <p class="text-sm text-gray-300">
                                Your beliefs (Q) exactly match reality (P). No extra surprise!
                            </p>
                        </div>

                        <div class="p-6 bg-gray-800/50 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚ö†Ô∏è</div>
                            <p class="font-bold text-yellow-400 mb-2">Small Difference</p>
                            <p class="mono text-2xl mb-2">D<sub>KL</sub> = 0.5</p>
                            <p class="text-sm text-gray-300">
                                Your beliefs are close to reality, but not perfect. Some extra surprise.
                            </p>
                        </div>

                        <div class="p-6 bg-gray-800/50 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚ùå</div>
                            <p class="font-bold text-red-400 mb-2">Big Difference</p>
                            <p class="mono text-2xl mb-2">D<sub>KL</sub> = 5.0</p>
                            <p class="text-sm text-gray-300">
                                Your beliefs are very wrong! Lots of extra surprise.
                            </p>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-lg">
                        <strong>Key Takeaway:</strong> KL Divergence measures how different two probability
                        distributions are. It's always ‚â• 0, and equals 0 only when the distributions are identical.
                        It's the "cost" of using the wrong distribution!
                    </p>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 9: Bringing It Back to ML -->
        <section class="section" id="section-9">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Step 9: Connecting Everything to <span class="highlight-pink">Machine Learning</span>
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Now let's bring all these concepts together and see exactly how they power neural network training!
                </p>

                <!-- THE COMPLETE JOURNEY -->
                <div class="glass-card p-8 mb-8 bg-gradient-to-br from-purple-900/20 to-pink-900/20">
                    <h3 class="text-3xl font-bold mb-8 text-center">The Complete Journey: From Concepts to ML Loss</h3>

                    <div class="space-y-6">
                        <!-- Step 1: Surprise -->
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-12 h-12 bg-yellow-500/20 border-2 border-yellow-500 rounded-full flex items-center justify-center text-2xl font-bold">1</div>
                            <div class="flex-1">
                                <h4 class="text-xl font-bold text-yellow-400 mb-2">Surprise: Quantifying Unexpectedness</h4>
                                <p class="text-gray-300 mb-2">
                                    <span class="mono bg-gray-800/50 px-2 py-1 rounded">-ln(p)</span> tells us how surprising an outcome is.
                                </p>
                                <p class="text-sm text-gray-400 italic">
                                    ‚Üí In ML: We use this as a proxy for gaining info about our distributions.
                                </p>
                            </div>
                        </div>

                        <div class="text-center text-2xl text-gray-600">‚Üì</div>

                        <!-- Step 2: Entropy -->
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-12 h-12 bg-purple-500/20 border-2 border-purple-500 rounded-full flex items-center justify-center text-2xl font-bold">2</div>
                            <div class="flex-1">
                                <h4 class="text-xl font-bold text-purple-400 mb-2">Entropy: Average Surprise from a Distribution</h4>
                                <p class="text-gray-300 mb-2">
                                    <span class="mono bg-gray-800/50 px-2 py-1 rounded">H(P) = Œ£ p(i) √ó (-ln p(i))</span> measures uncertainty in distribution P.
                                </p>
                                <p class="text-sm text-gray-400 italic">
                                    ‚Üí In ML: The correct output distribution's entropy is used as a representation of the truth. Since this is a one-hot label distribution, its entropy is always 0!
                                </p>
                            </div>
                        </div>

                        <div class="text-center text-2xl text-gray-600">‚Üì</div>

                        <!-- Step 3: Cross-Entropy -->
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-12 h-12 bg-pink-500/20 border-2 border-pink-500 rounded-full flex items-center justify-center text-2xl font-bold">3</div>
                            <div class="flex-1">
                                <h4 class="text-xl font-bold text-pink-400 mb-2">Cross-Entropy: Surprise with Wrong Beliefs</h4>
                                <p class="text-gray-300 mb-2">
                                    <span class="mono bg-gray-800/50 px-2 py-1 rounded">H(P, Q) = Œ£ p(i) √ó (-ln q(i))</span> measures surprise when reality is P but you believe Q.
                                </p>
                                <p class="text-sm text-gray-400 italic">
                                    ‚Üí In ML: This measures how surprised we are by the true labels given the model's predictions. It quantifies how different our predicted distribution is from the truth. This is our LOSS FUNCTION!
                                </p>
                            </div>
                        </div>

                        <div class="text-center text-2xl text-gray-600">‚Üì</div>

                        <!-- Step 4: KL Divergence -->
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-12 h-12 bg-blue-500/20 border-2 border-blue-500 rounded-full flex items-center justify-center text-2xl font-bold">4</div>
                            <div class="flex-1">
                                <h4 class="text-xl font-bold text-blue-400 mb-2">KL Divergence: The Extra Surprise</h4>
                                <p class="text-gray-300 mb-2">
                                    <span class="mono bg-gray-800/50 px-2 py-1 rounded">D<sub>KL</sub>(P||Q) = H(P,Q) - H(P)</span> isolates the "penalty" for Q being different from P.
                                </p>
                                <p class="text-sm text-gray-400 italic">
                                    ‚Üí In ML: This is what we're REALLY minimizing ‚Äî how different our predictions are from reality. Since H(P) = 0 for one-hot labels, minimizing cross-entropy = minimizing KL divergence!
                                </p>
                            </div>
                        </div>
                    </div>
                </div>

                <!-- THE ML SETUP -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">In Machine Learning Classification</h3>

                    <div class="grid md:grid-cols-2 gap-6 mb-8">
                        <div class="p-6 bg-blue-900/20 border-2 border-blue-500/30 rounded-lg">
                            <h4 class="font-bold text-blue-400 mb-4 text-center text-xl">Q: Model's Prediction</h4>
                            <p class="text-sm text-gray-300 mb-4 text-center">
                                The model's <span class="highlight-blue">"beliefs"</span> about what the answer is
                            </p>
                            <div class="space-y-2">
                                <div>
                                    <p class="text-sm">üê± Cat: <span class="mono text-blue-400">60%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-blue-500 h-3 rounded-full" style="width: 60%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm">üê∂ Dog: <span class="mono text-blue-400">30%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-blue-500 h-3 rounded-full" style="width: 30%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm">üê¶ Bird: <span class="mono text-blue-400">10%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-blue-500 h-3 rounded-full" style="width: 10%"></div>
                                    </div>
                                </div>
                            </div>
                            <div class="mt-4 p-3 bg-blue-900/30 rounded">
                                <p class="text-xs text-gray-300">
                                    The model outputs probabilities for each class. It's uncertain, making educated guesses.
                                </p>
                            </div>
                        </div>

                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg">
                            <h4 class="font-bold text-green-400 mb-4 text-center text-xl">P: Ground Truth</h4>
                            <p class="text-sm text-gray-300 mb-4 text-center">
                                <span class="highlight-green">"Reality"</span> ‚Äî the actual correct answer
                            </p>
                            <div class="space-y-2">
                                <div>
                                    <p class="text-sm font-bold">üê± Cat: <span class="mono text-green-400">100%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-green-500 h-3 rounded-full" style="width: 100%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm">üê∂ Dog: <span class="mono text-gray-400">0%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-gray-600 h-3 rounded-full" style="width: 0%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm">üê¶ Bird: <span class="mono text-gray-400">0%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-3">
                                        <div class="bg-gray-600 h-3 rounded-full" style="width: 0%"></div>
                                    </div>
                                </div>
                            </div>
                            <div class="mt-4 p-3 bg-green-900/30 rounded">
                                <p class="text-xs text-gray-300">
                                    One-hot encoded: 100% certain which class is correct. No uncertainty in the truth!
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="p-6 bg-gradient-to-r from-purple-900/30 to-pink-900/30 rounded-lg border-2 border-pink-500/30">
                        <p class="font-bold text-center text-2xl mb-4">The Training Goal</p>
                        <p class="text-center text-lg text-gray-200 leading-relaxed">
                            Make <span class="highlight-blue">Q</span> (model's predictions) match <span class="highlight-green">P</span> (ground truth).
                            <br>
                            In other words: <span class="highlight-yellow">minimize how different these distributions are!</span>
                        </p>
                    </div>
                </div>

                <!-- WHY CROSS-ENTROPY LOSS -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">Why Cross-Entropy Loss? The Complete Story</h3>

                    <div class="space-y-6">
                        <div class="step-box">
                            <p class="font-bold mb-3 text-xl">1Ô∏è‚É£ We need a measure of "wrongness"</p>
                            <p class="text-gray-300">
                                How do we quantify how wrong the model's prediction Q is compared to the truth P?
                                We need a single number that captures the difference between entire distributions.
                            </p>
                            <div class="mt-3 p-3 bg-gray-900/50 rounded">
                                <p class="text-sm">
                                    ‚ùå Can't just subtract probabilities<br>
                                    ‚ùå Can't just look at the correct class in isolation<br>
                                    ‚úÖ <span class="text-pink-400 font-bold">Enter cross-entropy!</span>
                                </p>
                            </div>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3 text-xl">2Ô∏è‚É£ Cross-entropy captures the surprise of wrong beliefs</p>
                            <p class="text-gray-300 mb-3">
                                Cross-entropy H(P, Q) tells us: "How surprised are we by reality (P) when we believe our model's predictions (Q)?"
                            </p>
                            <div class="p-4 bg-gray-900/50 rounded font-mono text-sm">
                                Loss = H(P, Q) = Œ£ [ p(i) √ó (-ln q(i)) ]
                            </div>
                            <p class="text-gray-400 text-sm mt-2">
                                For each class: reality's probability √ó surprise based on model's prediction
                            </p>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3 text-xl">3Ô∏è‚É£ What we REALLY want: minimize KL divergence</p>
                            <p class="text-gray-300 mb-3">
                                KL divergence D<sub>KL</sub>(P || Q) directly measures how different Q is from P. It's the perfect metric!
                            </p>
                            <div class="p-4 bg-gray-900/50 rounded font-mono text-sm">
                                D<sub>KL</sub>(P || Q) = H(P, Q) - H(P)
                            </div>
                            <p class="text-gray-400 text-sm mt-2">
                                "Extra surprise from wrong beliefs" = "Total surprise" - "Inherent surprise in reality"
                            </p>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3 text-xl">4Ô∏è‚É£ But H(P) is constant (and equals 0)!</p>
                            <p class="text-gray-300 mb-3">
                                Since our ground truth P is one-hot encoded (100% on correct class, 0% elsewhere):
                            </p>
                            <div class="p-4 bg-gray-900/50 rounded font-mono text-sm space-y-2">
                                <p>H(P) = <span class="text-green-400">1.0</span> √ó (-ln <span class="text-green-400">1.0</span>) + <span class="text-gray-500">0</span> √ó (-ln <span class="text-gray-500">0</span>) + ...</p>
                                <p>H(P) = 0 + 0 + ... = <span class="text-green-400 font-bold">0</span></p>
                            </div>
                            <p class="text-gray-400 text-sm mt-2">
                                Perfect certainty in labels = zero inherent uncertainty!
                            </p>
                        </div>

                        <div class="p-6 bg-gradient-to-r from-blue-900/40 to-purple-900/40 rounded-lg border-2 border-purple-500/50">
                            <p class="font-bold text-center text-2xl mb-4">The Beautiful Conclusion</p>
                            <div class="space-y-4 text-gray-200">
                                <div class="p-4 bg-gray-900/50 rounded font-mono">
                                    <p class="text-center mb-2">Since H(P) = 0:</p>
                                    <p class="text-center text-lg">
                                        <span class="text-yellow-400">D<sub>KL</sub>(P || Q)</span> =
                                        <span class="text-pink-400">H(P, Q)</span> -
                                        <span class="text-green-400">0</span>
                                    </p>
                                    <p class="text-center mt-2 text-xl font-bold">
                                        D<sub>KL</sub>(P || Q) = H(P, Q)
                                    </p>
                                </div>
                                <p class="text-center text-lg leading-relaxed">
                                    <span class="highlight-yellow">Minimizing cross-entropy</span> is <span class="font-bold">identical</span> to
                                    <span class="highlight-blue">minimizing KL divergence</span>!
                                    <br><br>
                                    We just compute H(P, Q) because it's simpler, but we're automatically minimizing how different
                                    our predictions are from reality.
                                </p>
                            </div>
                        </div>

                        <div class="step-box">
                            <p class="font-bold mb-3 text-xl">5Ô∏è‚É£ And it simplifies even further!</p>
                            <p class="text-gray-300 mb-3">
                                Because P is one-hot (only one class has p = 1, all others have p = 0), the sum collapses:
                            </p>
                            <div class="p-4 bg-gray-900/50 rounded font-mono text-sm space-y-2">
                                <p>H(P, Q) = <span class="text-green-400">p(Cat)</span> √ó (-ln q(Cat)) + <span class="text-gray-500">p(Dog)</span> √ó (-ln q(Dog)) + ...</p>
                                <p>H(P, Q) = <span class="text-green-400">1.0</span> √ó (-ln q(Cat)) + <span class="text-gray-500">0</span> √ó (-ln q(Dog)) + ...</p>
                                <p class="text-lg text-pink-400 font-bold">Loss = -ln(q<sub>correct</sub>)</p>
                            </div>
                            <p class="text-center text-lg text-yellow-400 mt-4">
                                Just the negative log probability the model assigned to the correct class!
                            </p>
                        </div>
                    </div>
                </div>

                <!-- WHAT THIS MEANS DURING TRAINING -->
                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold mb-6 text-center">What Happens During Training</h3>

                    <div class="grid md:grid-cols-3 gap-4 mb-6">
                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚úÖ</div>
                            <p class="font-bold text-green-400 mb-3">Good Prediction</p>
                            <p class="text-sm text-gray-300 mb-2">
                                Model gives <span class="mono text-lg">95%</span> to correct class
                            </p>
                            <div class="p-3 bg-gray-900/50 rounded mono text-sm">
                                Loss = -ln(0.95)<br>
                                = <span class="text-green-400 font-bold">0.051</span>
                            </div>
                            <p class="text-xs text-gray-400 mt-3">Low loss ‚úì</p>
                        </div>

                        <div class="p-6 bg-yellow-900/20 border-2 border-yellow-500/30 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚ö†Ô∏è</div>
                            <p class="font-bold text-yellow-400 mb-3">Mediocre Prediction</p>
                            <p class="text-sm text-gray-300 mb-2">
                                Model gives <span class="mono text-lg">50%</span> to correct class
                            </p>
                            <div class="p-3 bg-gray-900/50 rounded mono text-sm">
                                Loss = -ln(0.50)<br>
                                = <span class="text-yellow-400 font-bold">0.693</span>
                            </div>
                            <p class="text-xs text-gray-400 mt-3">Medium loss ‚ö†</p>
                        </div>

                        <div class="p-6 bg-red-900/20 border-2 border-red-500/30 rounded-lg text-center">
                            <div class="text-4xl mb-3">‚ùå</div>
                            <p class="font-bold text-red-400 mb-3">Bad Prediction</p>
                            <p class="text-sm text-gray-300 mb-2">
                                Model gives <span class="mono text-lg">5%</span> to correct class
                            </p>
                            <div class="p-3 bg-gray-900/50 rounded mono text-sm">
                                Loss = -ln(0.05)<br>
                                = <span class="text-red-400 font-bold">2.996</span>
                            </div>
                            <p class="text-xs text-gray-400 mt-3">High loss ‚úó</p>
                        </div>
                    </div>

                    <div class="p-6 bg-gradient-to-r from-purple-900/30 to-blue-900/30 rounded-lg border-2 border-blue-500/30">
                        <p class="font-bold text-center text-xl mb-4">During Training:</p>
                        <div class="space-y-3 text-gray-200">
                            <p>
                                1Ô∏è‚É£ Model makes predictions (outputs distribution Q)
                            </p>
                            <p>
                                2Ô∏è‚É£ We calculate cross-entropy loss = -ln(q<sub>correct</sub>)
                            </p>
                            <p>
                                3Ô∏è‚É£ Backpropagation adjusts weights to minimize this loss
                            </p>
                            <p>
                                4Ô∏è‚É£ Over time, model learns to assign higher probability to correct classes
                            </p>
                            <p class="text-yellow-400 text-lg font-bold text-center mt-4">
                                ‚Üí Loss decreases ‚Üí Predictions get closer to ground truth ‚Üí Model learns!
                            </p>
                        </div>
                    </div>
                </div>

                <div class="info-callout info-callout-success">
                    <p class="text-2xl mb-4">
                        <strong>The Full Picture:</strong>
                    </p>
                    <div class="text-lg text-gray-200 space-y-3">
                        <p>
                            <span class="highlight-yellow">Surprise</span> ‚Üí quantifies unexpectedness of outcomes
                        </p>
                        <p>
                            <span class="highlight-purple">Entropy</span> ‚Üí average surprise for a distribution (measures uncertainty)
                        </p>
                        <p>
                            <span class="highlight-pink">Cross-Entropy</span> ‚Üí average surprise when beliefs (Q) don't match reality (P)
                        </p>
                        <p>
                            <span class="highlight-blue">KL Divergence</span> ‚Üí the extra surprise from wrong beliefs = H(P,Q) - H(P)
                        </p>
                        <p>
                            <span class="highlight-green">For one-hot labels</span> ‚Üí H(P) = 0, so minimizing H(P,Q) = minimizing D<sub>KL</sub>
                        </p>
                        <p class="pt-4 border-t border-gray-600 text-xl font-bold text-center text-yellow-400">
                            That's why we use cross-entropy as our loss function!<br>
                            It perfectly measures how wrong our predictions are.
                        </p>
                    </div>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>


        <!-- SECTION 10: Interactive Final Demo -->
        <section class="section" id="section-10">
            <div class="max-w-5xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    Interactive <span class="highlight-pink">Playground</span>
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Now that you understand everything, play with this interactive demo to see it all in action!
                </p>

                <div class="glass-card p-8 mb-8">
                    <h3 class="text-2xl font-bold text-center mb-6">
                        Adjust the Model's Predictions
                    </h3>
                    <p class="text-center text-gray-300 mb-8">
                        The correct answer is <span class="highlight-green font-bold text-xl">Class C</span>.
                        Move the sliders to change the model's predictions and watch how all the metrics update!
                    </p>

                    <div class="grid md:grid-cols-2 gap-8 mb-8">
                        <!-- Ground Truth -->
                        <div class="p-6 bg-green-900/20 border-2 border-green-500/30 rounded-lg">
                            <h4 class="text-xl font-bold text-green-400 mb-4 text-center">Ground Truth (P)</h4>
                            <div class="space-y-3">
                                <div>
                                    <p class="text-sm mb-1">Class A: <span class="mono">0%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-gray-600 h-4 rounded-full" style="width: 0%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm mb-1">Class B: <span class="mono">0%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-gray-600 h-4 rounded-full" style="width: 0%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm mb-1 font-bold text-white">Class C (Correct!): <span class="mono">100%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-green-500 h-4 rounded-full" style="width: 100%"></div>
                                    </div>
                                </div>
                                <div>
                                    <p class="text-sm mb-1">Class D: <span class="mono">0%</span></p>
                                    <div class="w-full bg-gray-700/50 rounded-full h-4">
                                        <div class="bg-gray-600 h-4 rounded-full" style="width: 0%"></div>
                                    </div>
                                </div>
                            </div>
                        </div>

                        <!-- Model Prediction -->
                        <div class="p-6 bg-blue-900/20 border-2 border-blue-500/30 rounded-lg">
                            <h4 class="text-xl font-bold text-blue-400 mb-4 text-center">Model Prediction (Q)</h4>
                            <div class="space-y-4" id="prediction-controls">
                                <!-- Populated by JavaScript -->
                            </div>
                        </div>
                    </div>

                    <!-- Metrics Display -->
                    <div class="grid md:grid-cols-4 gap-4 mb-8">
                        <div class="p-4 bg-gray-800/50 rounded-lg text-center">
                            <p class="text-xs text-gray-400 mb-1">Entropy of P</p>
                            <p class="text-sm font-bold text-green-400 mb-1">H(P)</p>
                            <p class="number-display text-green-400" style="font-size: 1.5rem;" id="metric-entropy-p">0.000</p>
                            <p class="text-xs text-gray-400">nats</p>
                        </div>
                        <div class="p-4 bg-gray-800/50 rounded-lg text-center">
                            <p class="text-xs text-gray-400 mb-1">Cross-Entropy</p>
                            <p class="text-sm font-bold text-red-400 mb-1">H(P, Q)</p>
                            <p class="number-display text-red-400" style="font-size: 1.5rem;" id="metric-cross-entropy">1.386</p>
                            <p class="text-xs text-gray-400">nats</p>
                        </div>
                        <div class="p-4 bg-gray-800/50 rounded-lg text-center">
                            <p class="text-xs text-gray-400 mb-1">KL Divergence</p>
                            <p class="text-sm font-bold text-yellow-400 mb-1">D<sub>KL</sub>(P||Q)</p>
                            <p class="number-display text-yellow-400" style="font-size: 1.5rem;" id="metric-kl">1.386</p>
                            <p class="text-xs text-gray-400">nats</p>
                        </div>
                        <div class="p-4 bg-gray-800/50 rounded-lg text-center">
                            <p class="text-xs text-gray-400 mb-1">Simplified Loss</p>
                            <p class="text-sm font-bold text-pink-400 mb-1">-ln(q<sub>C</sub>)</p>
                            <p class="number-display text-pink-400" style="font-size: 1.5rem;" id="metric-simple">1.386</p>
                            <p class="text-xs text-gray-400">nats</p>
                        </div>
                    </div>

                    <div class="info-callout">
                        <ul class="space-y-2 text-sm">
                            <li>‚úÖ Set Class C close 100% - Watch all losses drop towards 0!</li>
                            <li>‚úÖ Make Class C very small - See the loss explode!</li>
                            <li>‚úÖ Notice that Cross-Entropy always equals KL Divergence (because H(P) = 0)</li>
                            <li>‚úÖ Verify that -ln(q<sub>C</sub>) always equals the Cross-Entropy</li>
                        </ul>
                    </div>
                </div>

                <div class="scroll-indicator">
                    <svg class="w-8 h-8 text-blue-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 14l-7 7m0 0l-7-7m7 7V3"></path>
                    </svg>
                </div>
            </div>
        </section>

        <!-- SECTION 11: Final Summary -->
        <section class="section" id="section-11">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-4xl md:text-5xl font-bold text-center mb-8">
                    The Complete <span class="highlight-purple">Picture</span>
                </h2>

                <p class="text-xl text-gray-300 text-center mb-12 leading-relaxed">
                    Let's review everything we've learned, step by step.
                </p>

                <div class="glass-card p-8 mb-8 bg-gradient-to-br from-purple-900/20 to-blue-900/20">
                    <div class="space-y-8">
                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">1Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-blue-400 text-lg mb-2">Neural networks output probability distributions</p>
                                    <p class="text-gray-300 text-sm">
                                        For each input, the model assigns a probability to every possible class.
                                        These probabilities represent the model's uncertainty and sum to 100%.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">2Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-green-400 text-lg mb-2">Ground truth is also a distribution</p>
                                    <p class="text-gray-300 text-sm">
                                        The correct answer is represented as a one-hot distribution: 100% on the correct class,
                                        0% on all others. This represents perfect certainty about what the answer is.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">3Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-yellow-400 text-lg mb-2">Surprise quantifies unexpectedness</p>
                                    <p class="text-gray-300 text-sm">
                                        For an event with probability p, the surprise is -ln(p). Rare events (low p) are
                                        very surprising. Common events (high p) are not surprising.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">4Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-purple-400 text-lg mb-2">Entropy is average surprise</p>
                                    <p class="text-gray-300 text-sm">
                                        Entropy H(P) measures the expected surprise when sampling from distribution P.
                                        It quantifies uncertainty: high entropy = unpredictable, low entropy = predictable.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">5Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-pink-400 text-lg mb-2">Cross-entropy accounts for wrong beliefs</p>
                                    <p class="text-gray-300 text-sm">
                                        Cross-entropy H(P, Q) measures the expected surprise when reality follows P but
                                        you expect Q. It's entropy plus the extra surprise from having incorrect beliefs.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">6Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-orange-400 text-lg mb-2">KL divergence isolates the difference</p>
                                    <p class="text-gray-300 text-sm">
                                        KL divergence D<sub>KL</sub>(P||Q) = H(P,Q) - H(P) is the extra surprise from Q being
                                        different from P. It measures how different two distributions are.
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">7Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-red-400 text-lg mb-2">For one-hot labels, cross-entropy = KL divergence</p>
                                    <p class="text-gray-300 text-sm">
                                        Since ground truth is one-hot encoded, H(P) = 0. Therefore, minimizing cross-entropy
                                        is identical to minimizing KL divergence. That's why we call it "cross-entropy loss."
                                    </p>
                                </div>
                            </div>
                        </div>

                        <div class="step-box">
                            <div class="flex gap-4">
                                <div class="text-3xl">8Ô∏è‚É£</div>
                                <div>
                                    <p class="font-bold text-cyan-400 text-lg mb-2">It simplifies beautifully</p>
                                    <p class="text-gray-300 text-sm">
                                        For one-hot labels, the entire cross-entropy formula simplifies to just -ln(q<sub>correct</sub>).
                                        The loss is simply: "how much probability did the model assign to the right answer?"
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="mt-12 p-8 bg-gradient-to-r from-blue-900/30 to-purple-900/30 rounded-lg border-2 border-purple-500/30">
                        <p class="text-2xl font-bold text-white text-center mb-4">
                            You Now Understand Loss!
                        </p>
                        <p class="text-gray-300 text-center text-lg">
                            Every time you see a neural network training and the loss decreasing, you now know
                            exactly what's happening: the model is adjusting its predictions to minimize the
                            KL divergence between its output distribution and the true distribution, which we
                            measure using cross-entropy. The model is learning to assign higher probability
                            to the correct answers!
                        </p>
                    </div>
                </div>
            </div>
        </section>

    </div>

    <script>
        // ==========================================
        // UTILITY FUNCTIONS
        // ==========================================
        function log(x) {
            if (x <= 0) return 0;
            return Math.log(x);
        }

        function calculateEntropy(distribution) {
            return distribution.reduce((sum, p) => {
                if (p <= 0) return sum;
                return sum - p * log(p);
            }, 0);
        }

        function calculateCrossEntropy(pDist, qDist) {
            let sum = 0;
            for (let i = 0; i < pDist.length; i++) {
                if (pDist[i] > 0 && qDist[i] > 0) {
                    sum += -pDist[i] * log(qDist[i]);
                } else if (pDist[i] > 0 && qDist[i] <= 0) {
                    return Infinity;
                }
            }
            return sum;
        }

        function normalize(values) {
            const sum = values.reduce((a, b) => a + b, 0);
            if (sum === 0) return values.map(() => 1 / values.length);
            return values.map(v => v / sum);
        }

        // ==========================================
        // PROGRESS BAR
        // ==========================================
        const progressBar = document.getElementById('progressBar');

        window.addEventListener('scroll', () => {
            const windowHeight = window.innerHeight;
            const documentHeight = document.documentElement.scrollHeight;
            const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
            const scrollPercent = (scrollTop / (documentHeight - windowHeight)) * 100;
            progressBar.style.width = scrollPercent + '%';
        });

        // ==========================================
        // SECTION VISIBILITY ANIMATION
        // ==========================================
        const sections = document.querySelectorAll('.section');

        const observerOptions = {
            threshold: 0.15,
            rootMargin: '0px 0px -100px 0px'
        };

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    entry.target.classList.add('visible');
                }
            });
        }, observerOptions);

        sections.forEach(section => {
            if (!section.classList.contains('visible')) {
                observer.observe(section);
            }
        });

        // ==========================================
        // INTERACTIVE FINAL DEMO (SECTION 11)
        // ==========================================
        const truthDist = [0, 0, 1, 0]; // Class C is correct
        let predictionDist = [0.25, 0.25, 0.25, 0.25];
        const classNames = ['Class A', 'Class B', 'Class C', 'Class D'];
        const barColors = ['#3b82f6', '#8b5cf6', '#10b981', '#f59e0b'];

        function initFinalDemo() {
            const controlsContainer = document.getElementById('prediction-controls');
            if (!controlsContainer) return;

            let controlsHTML = '';
            for (let i = 0; i < 4; i++) {
                const isCorrect = i === 2;
                controlsHTML += `
                    <div>
                        <div class="flex justify-between mb-2">
                            <span class="${isCorrect ? 'font-bold text-white' : ''}">${classNames[i]}</span>
                            <span class="mono text-sm" id="pred-value-${i}">${(predictionDist[i] * 100).toFixed(0)}%</span>
                        </div>
                        <input type="range" min="0" max="100" value="${predictionDist[i] * 100}"
                               class="w-full h-3 rounded-lg appearance-none cursor-pointer"
                               style="background: linear-gradient(90deg, #1e293b, ${barColors[i]});"
                               id="pred-slider-${i}"
                               oninput="updatePrediction(${i}, this.value)">
                        <div class="w-full bg-gray-700/50 rounded-full h-4 mt-2">
                            <div class="h-4 rounded-full transition-all"
                                 id="pred-bar-${i}"
                                 style="width: ${predictionDist[i] * 100}%; background: ${barColors[i]};"></div>
                        </div>
                    </div>
                `;
            }
            controlsContainer.innerHTML = controlsHTML;

            updateMetrics();
        }

        function updatePrediction(index, value) {
            predictionDist[index] = parseFloat(value) / 100;
            predictionDist = normalize(predictionDist);

            for (let i = 0; i < 4; i++) {
                const slider = document.getElementById(`pred-slider-${i}`);
                const valueDisplay = document.getElementById(`pred-value-${i}`);
                const bar = document.getElementById(`pred-bar-${i}`);

                const percent = predictionDist[i] * 100;
                slider.value = percent;
                valueDisplay.textContent = percent.toFixed(0) + '%';
                bar.style.width = percent + '%';
            }

            updateMetrics();
        }

        function updateMetrics() {
            const entropyP = calculateEntropy(truthDist);
            const crossEntropy = calculateCrossEntropy(truthDist, predictionDist);
            const kld = crossEntropy - entropyP;
            const simpleLoss = predictionDist[2] > 0 ? -log(predictionDist[2]) : Infinity;

            const entropyPElem = document.getElementById('metric-entropy-p');
            const crossEntropyElem = document.getElementById('metric-cross-entropy');
            const klElem = document.getElementById('metric-kl');
            const simpleElem = document.getElementById('metric-simple');

            if (entropyPElem) entropyPElem.textContent = entropyP.toFixed(3);
            if (crossEntropyElem) {
                crossEntropyElem.textContent = crossEntropy === Infinity ? '‚àû' : crossEntropy.toFixed(3);
            }
            if (klElem) {
                klElem.textContent = kld === Infinity ? '‚àû' : kld.toFixed(3);
            }
            if (simpleElem) {
                simpleElem.textContent = simpleLoss === Infinity ? '‚àû' : simpleLoss.toFixed(3);
            }
        }

        // ==========================================
        // INTERACTIVE SURPRISE DEMO (SECTION 5)
        // ==========================================
        function initSurpriseDemo() {
            const slider = document.getElementById('surprise-slider');
            const probDisplay = document.getElementById('surprise-prob-display');
            const valueDisplay = document.getElementById('surprise-value-display');
            const textDisplay = document.getElementById('surprise-text-display');

            if (!slider) return;

            function updateSurprise() {
                const p = slider.value / 100;
                const surprise = -log(p);

                probDisplay.textContent = `${slider.value}%`;
                valueDisplay.textContent = `${surprise.toFixed(3)} nats`;

                let text = '';
                let colorClass = '';
                if (surprise < 0.1) {
                    text = 'Barely surprised';
                    colorClass = 'text-green-400';
                } else if (surprise < 1.5) {
                    text = 'Moderately surprised';
                    colorClass = 'text-yellow-400';
                } else {
                    text = 'Very surprised!';
                    colorClass = 'text-red-400';
                }
                textDisplay.textContent = text;
                valueDisplay.className = `number-display ${colorClass}`;
            }

            slider.addEventListener('input', updateSurprise);
            updateSurprise(); // Initial call
        }

        // ==========================================
        // INTERACTIVE ENTROPY DEMO (SECTION 6)
        // ==========================================
        function initEntropyDemo() {
            const slider = document.getElementById('entropy-slider');
            if (!slider) return;

            const probDisplay = document.getElementById('entropy-prob-display');
            const headsText = document.getElementById('heads-prob-text');
            const tailsText = document.getElementById('tails-prob-text');
            const headsBar = document.getElementById('heads-prob-bar');
            const tailsBar = document.getElementById('tails-prob-bar');
            const entropyValue = document.getElementById('entropy-value-display');
            const entropyText = document.getElementById('entropy-text-display');

            function updateEntropy() {
                const headsProb = slider.value / 100;
                const tailsProb = 1 - headsProb;
                const entropy = calculateEntropy([headsProb, tailsProb]);

                probDisplay.textContent = `${slider.value}% Heads`;
                headsText.textContent = `${(headsProb * 100).toFixed(0)}%`;
                tailsText.textContent = `${(tailsProb * 100).toFixed(0)}%`;
                headsBar.style.width = `${headsProb * 100}%`;
                tailsBar.style.width = `${tailsProb * 100}%`;
                entropyValue.textContent = entropy.toFixed(3);

                if (entropy > 0.65) {
                    entropyText.textContent = "High uncertainty - the outcome is very unpredictable!";
                    entropyValue.className = "number-display text-purple-400";
                } else if (entropy > 0.3) {
                    entropyText.textContent = "Some uncertainty - one outcome is more likely.";
                    entropyValue.className = "number-display text-blue-400";
                } else {
                    entropyText.textContent = "Low uncertainty - the outcome is almost certain!";
                    entropyValue.className = "number-display text-green-400";
                }
            }

            slider.addEventListener('input', updateEntropy);
            updateEntropy();
        }

        // Made by JPeggysus on Github
        // ==========================================
        // INITIALIZE ON LOAD
        // ==========================================
        window.addEventListener('load', () => {
            initFinalDemo();
            initSurpriseDemo();
            initEntropyDemo();
        });
    </script>
</body>
</html>
